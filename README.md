
# ğŸ—£ï¸ Multilingual Hate Speech Detection using mBERTâ€“BiLSTM

An end-to-end deep learning project that leverages a hybrid **mBERTâ€“BiLSTM** architecture to detect hate speech in **Bengali** and **Indonesian** social media content. The system supports multilingual input, fine-tuned transformer models, and offers real-time classification â€” ideal for moderation and abuse monitoring platforms.

---

## ğŸ“Œ Table of Contents

* [About the Project](#about-the-project)
* [Datasets](#datasets)
* [Problem Statement](#problem-statement)
* [Methodology](#methodology)
* [Model Architecture](#model-architecture)
* [Tech Stack](#tech-stack)
* [How to Run the Project](#how-to-run-the-project)
* [Results](#results)
* [Future Work](#future-work)

---

<a name="about-the-project"></a>
## ğŸ“– About the Project

The surge in multilingual hate speech on social media platforms demands intelligent systems capable of handling code-mixed and low-resource language data. This project proposes a **hybrid mBERTâ€“BiLSTM model** fine-tuned on Bengali and Indonesian datasets to classify text as hateful or non-hateful. It ensures real-time inference, robust evaluation, and scalable deployment potential.

---

<a name="datasets"></a>
## ğŸ“Š Datasets

### Bengali Hate Speech Dataset (Kaggle)

* \~30,000 annotated comments from Facebook & YouTube
* 2 classes:

  * `1` â†’ Hate Speech
  * `0` â†’ Non-Hate
* Challenges: Slang, transliteration, code-mixing
  ğŸ”— [Bengali Dataset Link](https://www.kaggle.com/datasets/naurosromim/bengali-hate-speech-dataset)

### Indonesian Hate Speech Superset (Hugging Face)

* 14,306 Twitter posts from merged datasets
* Columns: `text`, `label`, `source`, `annotators`
* Balanced binary labels
  ğŸ”— [Indonesian Dataset Link](https://huggingface.co/datasets/manueltonneau/indonesian-hate-speech-superset)

---

<a name="problem-statement"></a>
## â“ Problem Statement

Design a robust multilingual hate speech detection model capable of identifying harmful content across **low-resource languages**. The system should generalize across linguistic variations and be suitable for **content moderation tools**.

---

<a name="methodology"></a>
## ğŸ” Methodology

### Preprocessing:

* Cleaning: Lowercase, remove URLs/usernames, punctuation
* Tokenization: `bert-base-multilingual-cased`
* Padding/truncation to 128 tokens
* Stratified 80:20 train-test split

### Training:

* Binary classification
* Model: mBERT + 2-layer BiLSTM (128 hidden units)
* Optimizer: AdamW (LR = 2e-5)
* Loss: Weighted CrossEntropy
* Epochs: 5
* Batch Size: 16

### Inference:

* Real-time text prediction
* Text â†’ Preprocess â†’ Tokenize â†’ Predict â†’ Label

---

<a name="model-architecture"></a>
## ğŸ§  Model Architecture: mBERTâ€“BiLSTM

| **Component**       | **Description**                                                |
| ------------------- | -------------------------------------------------------------- |
| **Base Model**      | `bert-base-multilingual-cased` (fine-tuned)                    |
| **Embedding Size**  | 768                                                            |
| **BiLSTM**          | 2 layers Ã— 128 hidden units (â†’ 256 after bidirectional concat) |
| **Dropout**         | 0.3 (applied after BiLSTM)                                     |
| **Classifier Head** | Dense layer (Linear 256 â†’ 2 classes)                           |
| **Loss Function**   | Weighted `CrossEntropyLoss`                                    |
| **Optimizer**       | `AdamW` with learning rate of `2e-5`                           |
| **Batch Size**      | 16                                                             |
| **Epochs**          | 5                                                              |

âœ… F1-Score (Bengali): **85.67%**

âœ… F1-Score (Indonesian): **82.08%**

---

<a name="tech-stack"></a>
## ğŸ› ï¸ Tech Stack

| Layer         | Tools/Frameworks                   |
| ------------- | ---------------------------------- |
| Model         | PyTorch, Hugging Face Transformers |
| Tokenization  | WordPiece, Attention Masks         |
| Data Handling | Pandas, NumPy                      |
| Evaluation    | Scikit-learn, Matplotlib, Seaborn  |

---

<a name="how-to-run-the-project"></a>
## âš™ï¸ How to Run the Project

### ğŸ” Clone the repo:

```bash
git clone https://github.com/PRIYANGA-SELVAPERUMAL/Multilingual_Hate_Speech_Detection.git
```

### ğŸ Create and activate virtual environment:

```bash
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows
```

### ğŸ“¦ Install dependencies:

```bash
pip install -r requirements.txt
```

### ğŸš€ Run notebook or inference script:

Launch `.ipynb` files for Bengali/Indonesian or run `main.py` for terminal-based inference.

---

<a name="results"></a>
## âœ… Results

| Metric    | Bengali | Indonesian |
| --------- | ------- | ---------- |
| Accuracy  | 89.95%  | 83.61%     |
| Precision | 81.62%  | 76.33%     |
| Recall    | 90.15%  | 88.76%     |
| F1-Score  | 85.67%  | 82.08%     |
| AUCâ€“ROC   | 0.9595  | 0.9085     |

* ğŸ“Š Confusion Matrices and AUC plots available in the results folder
* ğŸ” Sample predictions included for real-world evaluation

---

<a name="future-enhancements"></a>
## ğŸ”­ Future Enhancements

*  Add attention mechanism over BiLSTM outputs
*  Integrate XAI for explainable predictions
*  API-based deployment (Flask / FastAPI)
*  Mobile/web interface using Streamlit or React
*  Extend support to more languages (Tamil, Hindi, etc.)
*  Experiment with newer models like XLM-R and mT5



